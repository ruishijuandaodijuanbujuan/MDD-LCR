{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "import h5py\n",
    "import scipy.io as scio\n",
    "import argparse, os\n",
    "import math, random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "\n",
    "class DatasetFromHdf5(data.Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super(DatasetFromHdf5, self).__init__()\n",
    "        hf = h5py.File(file_path)\n",
    "        self.data = hf.get('data')\n",
    "        self.target = hf.get('label')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.data[index,:,:,:]).float(), torch.from_numpy(self.target[index,:,:,:]).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "\n",
    "class DatasetFrommat(data.Dataset):\n",
    "    def __init__(self, data_file_path, gt_file_path):\n",
    "        super(DatasetFrommat, self).__init__()\n",
    "        data_file = scio.loadmat(data_file_path)\n",
    "        gt_file = scio.loadmat(gt_file_path)\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        for i in range(1,29391): #2671：取决于实际训练RB数+1\n",
    "            self.data.append(data_file[f'fre_LS_GIGI{str(i)}_123'])\n",
    "            self.target.append(gt_file[f'H_gt{str(i)}_123'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.from_numpy(self.data[index]).float()\n",
    "        label = torch.from_numpy(self.target[index]).float()\n",
    "        label_magnitude = torch.sqrt(torch.pow(label[0, :, :, :],2) + torch.pow(label[1, :, :, :],2))\n",
    "        return  data, label, label_magnitude\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "class Dataset_eval(data.Dataset):\n",
    "    def __init__(self, data_file_path, length):\n",
    "        super(Dataset_eval, self).__init__()\n",
    "        data_file = scio.loadmat(data_file_path)\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        for i in range(1,201): #179：取决于实际测试RB数+1\n",
    "            for j in range(0,16):\n",
    "                self.data.append(data_file[f'fre_LS_GIGI{str(i)}_{str(2*j)}'])\n",
    "                self.target.append(f'fre_LS_GIGI{str(i)}_{str(2*j)}')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.data[index]).float(), self.target[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  4004\n",
      "===> Loading datasets\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/great80/data/train_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/dell/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/great80/data/train_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e0e455d2597e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===> Loading datasets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFrommat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/great80/data/train_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/home/great80/data/label_train_H_gt_RB_mixSNR_1_4_12_3D_3.mat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#路径修改\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-65cfb030e515>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_file_path, gt_file_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetFrommat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mgt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dell/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \"\"\"\n\u001b[1;32m    206\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dell/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dell/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/great80/data/train_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat'"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch SRResNet\")\n",
    "parser.add_argument(\"--batchSize\", type=int, default=32, help=\"training batch size\")\n",
    "parser.add_argument(\"--nEpochs\", type=int, default=480, help=\"number of epochs to train for\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"Learning Rate. Default=1e-4\")\n",
    "parser.add_argument(\"--step\", type=int, default=40, help=\"Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=500\")\n",
    "parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda?\")\n",
    "parser.add_argument(\"--resume\", default=\"\", type=str, help=\"Path to checkpoint (default: none)\")\n",
    "parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")\n",
    "parser.add_argument(\"--threads\", type=int, default=0, help=\"Number of threads for data loader to use, Default: 1\")\n",
    "parser.add_argument(\"--pretrained\", default=\"\", type=str, help=\"path to pretrained model (default: none)\")\n",
    "parser.add_argument(\"--gpus\", default=\"1\", type=str, help=\"gpu ids (default: 0)\")\n",
    "\n",
    "global opt, model, netContent\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10\"\"\"\n",
    "    lr = opt.lr * (0.1 ** (epoch // opt.step))\n",
    "    return lr \n",
    "\n",
    "def save_checkpoint(model, epoch, check_location):\n",
    "    model_out_path = \"saved_models/\" + check_location + \"/model_epoch_{}.pth\".format(epoch)\n",
    "    state = {\"epoch\": epoch ,\"model\": model}\n",
    "    if not os.path.exists(\"saved_models/\" + check_location):\n",
    "        os.makedirs(\"saved_models/\" + check_location)\n",
    "\n",
    "    torch.save(state, model_out_path)\n",
    "\n",
    "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cuda = True\n",
    "\n",
    "opt.seed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "torch.cuda.manual_seed(opt.seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "print(\"===> Loading datasets\")\n",
    "train_set = DatasetFrommat('/home/great80/data/train_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat', '/home/great80/data/label_train_H_gt_RB_mixSNR_1_4_12_3D_3.mat') #路径修改\n",
    "\n",
    "\n",
    "test_dataset_location = '/home/great80/data/test_fre_LS_GIGI_mixSNR_1_4_12_3D_3.mat'#路径修改\n",
    "test_dataset_gt_location = '/home/great80/data/label_test_H_gt_RB_mixSNR_1_4_12_3D_3.mat'#路径修改\n",
    "labels = scio.loadmat(test_dataset_gt_location)\n",
    "output = 'out_fre_LS_GIGI_RES_mixSNR_3D_3' #可以不改\n",
    "\n",
    "evaldataset = Dataset_eval(test_dataset_location, 200)    #178：按照实际测试RB数修改\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 12, 2])\n",
      "torch.Size([2, 64, 12, 2])\n",
      "torch.Size([64, 12, 2])\n",
      "torch.Size([2, 64, 12, 2])\n",
      "fre_LS_GIGI1_0\n"
     ]
    }
   ],
   "source": [
    "training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=8, shuffle=True)#, collate_fn=AlignCollate())\n",
    "eval_data_loader = DataLoader(dataset=evaldataset, batch_size=8, shuffle=True)#, collate_fn=AlignCollate_eval())\n",
    "\n",
    "print(train_set.__getitem__(0)[0].shape)\n",
    "print(train_set.__getitem__(0)[1].shape)\n",
    "print(train_set.__getitem__(0)[2].shape)\n",
    "print(evaldataset.__getitem__(0)[0].shape)\n",
    "print(evaldataset.__getitem__(0)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(model, eval_data_loader, labels, snr):\n",
    "    model.eval()\n",
    "    result_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(eval_data_loader, 1):\n",
    "            im_input = batch[0]\n",
    "            target = batch[1]\n",
    "#             im_input = im_input.permute(0,4,1,2,3).reshape(im_input.shape[0]*8,7,2,64,56).permute(0,2,3,4,1)\n",
    "            im_input = im_input[:,:,:,0:12:4,0:2:2].cuda() #SRCNN:im_input[:,:,:,0:12:1,0:2:1].cuda() #EDSR:im_input[:,:,:,0:12:4,0:2:2].cuda()\n",
    "\n",
    "            out = model(im_input)[0]\n",
    "\n",
    "#             out = out.permute(0,4,1,2,3).reshape(im_input.shape[0]//8,4*14,2,64,56).permute(0,2,3,4,1)\n",
    "            out = out.cpu().numpy()\n",
    "            for i in range(len(out)):\n",
    "                result_dict[target[i]] = out[i]    \n",
    "                \n",
    "#     mse = np.zeros((64*56,178))\n",
    "#     for i in range(1,179):\n",
    "#         out = result_dict[f'fre_LS_GIGI{i}_{snr}']\n",
    "#         label = labels[f'H_gt{i}_{snr}']\n",
    "#         out_complex = out[0] + out[1] * 1j\n",
    "#         label_complex = label[0,:,:,:] + label[1,:,:,:] * 1j\n",
    "#         for j in range(56):\n",
    "#             for k in range(64):\n",
    "#                 mse[j*64+k,i-1] = np.square(np.linalg.norm(out_complex[k,j,:]-label_complex[k,j,:])) /  np.square(np.linalg.norm(label_complex[k,j]))\n",
    "#     mse2 = np.sum(mse, 0) / 64 / 56\n",
    "#     mse3 = np.sum(mse2)/178\n",
    "    mse = np.zeros(201) #201：取决于实际测试RB数+1\n",
    "    for i in range(1,201): #201：取决于实际测试RB数+1\n",
    "        out = result_dict[f'fre_LS_GIGI{i}_{snr}']\n",
    "        label = labels[f'H_gt{i}_{snr}']\n",
    "        out_complex = out[0] + out[1] * 1j\n",
    "        label_complex = label[0,:,:,:] + label[1,:,:,:] * 1j\n",
    "        out_complex = out_complex.flatten()\n",
    "        label_complex = label_complex.flatten()\n",
    "        mse[i] = np.square(np.linalg.norm(out_complex-label_complex)) /  np.square(np.linalg.norm(label_complex))\n",
    "    mse3 = np.sum(mse)/200 #200：取决于实际测试RB数\n",
    "    return mse3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_conv(in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
    "    if not isinstance(kernel_size, int):\n",
    "        padding = [(i - 1) // 2 for i in kernel_size]\n",
    "    else:\n",
    "        padding = (kernel_size - 1) // 2\n",
    "    return nn.Conv3d(\n",
    "        in_channels, out_channels, (kernel_size,3,3), stride,\n",
    "        padding=(padding,1,1), bias=bias)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv, n_feats, kernel_size, stride,\n",
    "        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            m.append(conv(n_feats, n_feats, kernel_size[i], stride, bias=bias))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm3d(n_feats))\n",
    "            if i == 0:\n",
    "                m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "class PixelShuffle3d(nn.Module):\n",
    "    '''\n",
    "    This class is a 3d version of pixelshuffle.\n",
    "    '''\n",
    "    def __init__(self, scale):\n",
    "        '''\n",
    "        :param scale: upsample scale\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, channels, in_depth, in_height, in_width = input.size()\n",
    "        nOut = channels // (self.scale**2 )\n",
    "#         nOut = channels // self.scale\n",
    "\n",
    "        out_depth = in_depth\n",
    "        out_height = in_height * self.scale\n",
    "        out_width = in_width * self.scale \n",
    "#         out_width = in_width * self.scale * 2\n",
    "\n",
    "        input_view = input.contiguous().view(batch_size, nOut, self.scale, self.scale, in_depth, in_height, in_width)\n",
    "\n",
    "        output = input_view.permute(0, 1, 4, 5, 2, 6, 3).contiguous()\n",
    "\n",
    "        return output.view(batch_size, nOut, out_depth, out_height, out_width)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_channels=2, base_filter=16, n_resblocks=2, upscale_factor=1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        kernel_size_res = [3,3] \n",
    "        stride = 1\n",
    "        act = nn.ReLU(True)\n",
    "        n_resblocks = n_resblocks\n",
    "        \n",
    "#         m_head = [default_conv(num_channels, base_filter, 5)]\n",
    "#         self.head = nn.Sequential(   \n",
    "#             nn.ConvTranspose3d(2, base_filter//2, (1,2,2), stride=(1,2,2)),\n",
    "#             nn.BatchNorm3d(base_filter//2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ConvTranspose3d(base_filter//2, base_filter, (1,2,2), stride=(1,2,2)),\n",
    "#             nn.BatchNorm3d(base_filter),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             )\n",
    "        # define body module\n",
    "#         m_body = [\n",
    "#             ResBlock(\n",
    "#                 default_conv, base_filter, kernel_size_res, stride, act=act, res_scale=1\n",
    "#             ) for _ in range(n_resblocks)\n",
    "#         ]\n",
    "#         self.body = nn.Sequential(*m_body)\n",
    "        \n",
    "#         m_tail = []\n",
    "#         m_tail.append(default_conv(base_filter, 2, 5))\n",
    "#         self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "########### REFINED NET\n",
    "        self.tail = nn.Sequential(   \n",
    "            nn.ConvTranspose3d(base_filter, base_filter//2, (1,2,2), stride=(1,2,2)),\n",
    "            nn.BatchNorm3d(base_filter//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(base_filter//2, 2, (1,2,1), stride=(1,2,1)),\n",
    "            \n",
    "#             nn.ConvTranspose3d(base_filter, 2, (1,2,2), stride=(1,2,2)),\n",
    "            \n",
    "#             default_conv(base_filter, base_filter//2, 5), \n",
    "#             PixelShuffle3d(2),\n",
    "#             default_conv(base_filter // 2, base_filter // 2, 5),\n",
    "#             PixelShuffle3d(2)\n",
    "            \n",
    "            )\n",
    "    \n",
    "        # define body module\n",
    "        m_body = [\n",
    "            ResBlock(\n",
    "                default_conv, base_filter, kernel_size_res, stride, act=act, res_scale=1\n",
    "            ) for _ in range(n_resblocks)\n",
    "        ]\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "        \n",
    "        self.head = nn.Sequential(default_conv(2, base_filter, 5),\n",
    "#             nn.BatchNorm3d(base_filter),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.body(x) + x\n",
    "        x = self.tail(x)\n",
    "        x_magnitude = torch.sqrt(torch.pow(x[:, 0, :, :],2) + torch.pow(x[:, 1, :, :],2))\n",
    "\n",
    "        return x, x_magnitude\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SRCNN(torch.nn.Module):\n",
    "    def __init__(self, num_channels=2, base_filter=16, upscale_factor=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Conv3d(in_channels=num_channels, out_channels=base_filter, kernel_size=5, stride=1, padding = 2, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels=base_filter, out_channels=base_filter, kernel_size=5, stride=1, padding= 2, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels=base_filter, out_channels=num_channels, kernel_size=5, stride=1, padding= 2, bias=True),\n",
    "\n",
    "#             nn.Conv3d(in_channels=base_filter, out_channels=base_filter // 2, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv3d(in_channels=base_filter // 2, out_channels=num_channels * (upscale_factor ** 2), kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            #             nn.PixelShuffle(upscale_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x_magnitude = torch.sqrt(torch.pow(x[:, 0, :, :],2) + torch.pow(x[:, 1, :, :],2))\n",
    "\n",
    "        return x, x_magnitude \n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 12, 2])\n",
      "torch.Size([4, 2, 64, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "model = SRCNN(num_channels=2, base_filter=16, upscale_factor=1) #SRCNN\n",
    "# print(model)\n",
    "print(model(torch.ones(4,2,64,12,2))[1].shape)\n",
    "print(model(torch.ones(4,2,64,12,2))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 12, 2])\n",
      "torch.Size([4, 2, 64, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_channels=2, base_filter=16, n_resblocks=4) #EDSR \n",
    "# print(model)\n",
    "print(model(torch.ones(4,2,64,3,1))[1].shape)\n",
    "print(model(torch.ones(4,2,64,3,1))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expeiment_name = 'SNS/20230809_eCNN_RN' #随你喜欢改，最好每次训练都改个名字\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch SRResNet\")\n",
    "parser.add_argument(\"--batchSize\", type=int, default=32, help=\"training batch size\")\n",
    "parser.add_argument(\"--nEpochs\", type=int, default=480, help=\"number of epochs to train for\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Learning Rate. Default=1e-4\")\n",
    "parser.add_argument(\"--step\", type=int, default=40, help=\"Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=500\")\n",
    "parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use cuda?\")\n",
    "parser.add_argument(\"--resume\", default=\"\", type=str, help=\"Path to checkpoint (default: none)\")\n",
    "parser.add_argument(\"--start-epoch\", default=1, type=int, help=\"Manual epoch number (useful on restarts)\")\n",
    "parser.add_argument(\"--threads\", type=int, default=0, help=\"Number of threads for data loader to use, Default: 1\")\n",
    "parser.add_argument(\"--pretrained\", default=\"\", type=str, help=\"path to pretrained model (default: none)\")\n",
    "parser.add_argument(\"--gpus\", default=\"4\", type=str, help=\"gpu ids (default: 0)\")\n",
    "\n",
    "global opt, model, netContent\n",
    "opt = parser.parse_args(args=[])\n",
    "print(opt)\n",
    "\n",
    "\"\"\"tensorboard setting\"\"\"\n",
    "writer = SummaryWriter(f'./saved_models/{expeiment_name}')\n",
    "\n",
    "print(\"===> Building model\")\n",
    "\n",
    "model = Net(num_channels=2, base_filter=8, n_resblocks=4) #eCNN-RN\n",
    "# model = SRCNN(num_channels=2, base_filter=8, upscale_factor=1) #SRCNN\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "print(\"===> Setting GPU\")\n",
    "model = model.cuda()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "if opt.resume:\n",
    "    if os.path.isfile(opt.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(opt.resume))\n",
    "        checkpoint = torch.load(opt.resume)\n",
    "        opt.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        model.load_state_dict(checkpoint[\"model\"].state_dict())\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(opt.resume))\n",
    "\n",
    "# optionally copy weights from a checkpoint\n",
    "if opt.pretrained:\n",
    "    if os.path.isfile(opt.pretrained):\n",
    "        print(\"=> loading model '{}'\".format(opt.pretrained))\n",
    "        weights = torch.load(opt.pretrained)\n",
    "        model.load_state_dict(weights['model'].state_dict())\n",
    "    else:\n",
    "        print(\"=> no model found at '{}'\".format(opt.pretrained))\n",
    "\n",
    "print(\"===> Setting Optimizer\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "print(\"===> Training\")\n",
    "i = 0\n",
    "num_print = 4\n",
    "val_print = 334\n",
    "mse_best = 100\n",
    "running_loss = 0.0\n",
    "running_loss_mse = 0.0\n",
    "running_loss_magnitude = 0.0\n",
    "for epoch in range(opt.start_epoch, opt.nEpochs + 1):\n",
    "\n",
    "#     lr = adjust_learning_rate(optimizer, epoch-1)\n",
    "\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group[\"lr\"] = lr\n",
    "\n",
    "#     print(\"Epoch={}, lr={}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "\n",
    "        input, target, target_m = batch[0], batch[1], batch[2]\n",
    "#         print(input.shape)\n",
    "#         input = input.permute(0,4,1,2,3).reshape(input.shape[0]*8,7,2,64,56).permute(0,2,3,4,1)\n",
    "\n",
    "        input = input[:,:,:,0:12:4,0:2:2].cuda() #SRCNN: input[:,:,:,0:12:1,0:2:1].cuda() #EDSR: input[:,:,:,0:12:4,0:2:2].cuda()\n",
    "        target = target.cuda()\n",
    "        target_m = target_m.cuda()\n",
    "\n",
    "        output, output_m = model(input)\n",
    "        \n",
    "#         output_m = output_m.permute(0,3,1,2).reshape(input.shape[0]//8,4*14,64,56).permute(0,2,3,1)\n",
    "#         output = output.permute(0,4,1,2,3).reshape(input.shape[0]//8,4*14,2,64,56).permute(0,2,3,4,1)\n",
    "        loss_mse = criterion(output, target)\n",
    "        loss_magnitude = criterion(output_m, target_m)\n",
    "        loss = loss_mse + 0.1*loss_magnitude\n",
    "        \n",
    "        running_loss += loss_mse.item()\n",
    "        running_loss += loss_magnitude.item()\n",
    "        running_loss_mse += loss_mse.item()\n",
    "        running_loss_magnitude += loss_magnitude.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i % num_print == 0:\n",
    "            writer.add_scalar('train/Loss',  running_loss / num_print, i)\n",
    "            writer.add_scalar('train/Loss_mse',  running_loss_mse / num_print, i)\n",
    "            writer.add_scalar('train/Loss_magnitude',  running_loss_magnitude / num_print, i)\n",
    "            print(\"Epoch[{}]({}/{}): Loss: {:.5} Loss_x: {:.5} Loss_m: {:.5}\".format(epoch, iteration, len(training_data_loader), running_loss/num_print, running_loss_mse/num_print, running_loss_magnitude/num_print))\n",
    "            running_loss = 0.0\n",
    "            running_loss_mse = 0.0\n",
    "            running_loss_magnitude = 0.0\n",
    "        if i % val_print == 0:\n",
    "            mse = validation(model, eval_data_loader, labels,30)\n",
    "            print(\"Epoch[{}]: val mse: {:.5}\".format(epoch, mse))\n",
    "            writer.add_scalar('train/valmse',  mse, i)\n",
    "            if mse < mse_best:\n",
    "                save_checkpoint(model, i, expeiment_name)\n",
    "                mse_best = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('saved_models/SNS/20230809_eCNN_RN/model_epoch_1578150.pth')['model'].state_dict())\n",
    "model.cuda()\n",
    "\n",
    "val_snr_dict = {}\n",
    "for i in range(0, 32, 2):\n",
    "    mse = validation(model, eval_data_loader, labels, i)\n",
    "    val_snr_dict[i] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 16):\n",
    "    print(f'{2*i} \\t {val_snr_dict[2*i]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
